{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac863b0a",
   "metadata": {},
   "source": [
    "이번 섹션에선 신경망 언어 모델의 시초인 **피드 포워드 신경망 언어 모델(Feed Forward Neural Network Language Model)**에 대해서 학습한다.\n",
    "\n",
    "# 1. 기존 N-gram 언어 모델의 한계\n",
    "---\n",
    "\n",
    "언어 모델은 문장에 확률을 할당하는 모델이며, 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다. 예를 들어,\n",
    "\n",
    "An adorable little boy is spreading ____\n",
    "\n",
    "위와 같은 문장이 있다고 해보자. N-gram 언어 모델은 언어 모델링에 바로 앞 n-1개의 단어들을 참고한다. 예를 들어서 4-gram 언어 모델이라고 가정하자. 그럼 밑줄의 앞 3단어 boy is spreading을 보고 밑줄을 파악해야한다. 확률 식으로는 다음과 같이 쓸 수 있다.\n",
    "\n",
    "$$\n",
    "p(\\text{w}|\\text{boy is spreading}) = \\frac{\\text{count(boy is spreading w)}}{\\text{count(boy is spreading)}}\n",
    "$$\n",
    "\n",
    "만약 w가 smiles이란 단어였다고 해보자. 그리고 1000개의 boy is spreading의 말뭉치들이 있는 데이터에 smiles이 붙은게 100개 있다고 하면, 확률은 0.1이 될 것이다. 역으로 w가 smiles인데 내가 가진 말뭉치 데이터에서 한번도 관측된 적이 없다고 한다면, 확률이 0이 되면서 올바른 예측을 할 수 없다. 그러나 실제로는 가능한 일이다. 따라서 적절한 모델링이 되지 않음을 알 수 있고 이것을 n-gram 언어 모델의 **희소 문제(sparsity problem)**이라고 한다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 2. 단어의 의미적 유사성\n",
    "--- \n",
    "\n",
    "희소 문제는 **기계가 단어의 의미적 유사성을 알수 있다면 해결할 수 있는 문제**이다. 만약 내가 A라는 동사를 안다고 하자. 그리고 나는 사과를 A했다. 라고 의미를 알고 있다고 하자. 그리고 B가 A와 유사한 의미를 가지고 있다고만 배웠으며 한번도 B를 사용하는 것을 본 적 없다고 하자. 그래도 나는 자연스럽게 '나는 사과를 A했다'라는 문장을 **'나는 사과를 B했다.'** 라고 표현할 수 있을 것이다. 단지 **단어의 의미적 유사성을 파악**했다는 것만으로 말이다. \n",
    "\n",
    "그러나 A와 B의 유사도를 학습하지 않고, 인터넷에서 B로 쓴 것을 본 적이 없어서 통계적 기반으로는 확률 $p(\\text{B|나는 사과를 __했다.}) = 0$이 됬다고 하자. 이것은 유사도를 학습하지 못하는 N-gram이 방식을 얘기하는데, 이럴 경우엔 죽었다 깨어나도 '나는 사과를 B했다'라는 문장을 예측할 수 없다. \n",
    "\n",
    "만약 언어 모델 또한 단어의 의미적 유사성을 학습할 수 있도록 설계한다면, **훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측**을 할 수 있다. 그리고 *이러한 아이디어를 반영한 언어 모델이 신경망 언어 모델 NNLM*입니다. 그리고 이 아이디어는 *단어 벡터 간 유사도를 구할 수 있는 벡터를 얻어내는 워드 임베딩(word embedding) 의 아이디어*이기도 하다. \n",
    "\n",
    "이제 NNLM이 어떻게 훈련 과정에서 단어의 유사도를 학습할 수 있는지 알아보자.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 3. 피드 포워드 신경망 언어 모델(NNLM)\n",
    "--- \n",
    "\n",
    "여기서 부턴 책의 예제를 그대로 쓰도록 하겠다.\n",
    "\n",
    "NNLM이 언어 모델링을 학습하는 과정을 보겠습니다. 이해를 위해 간소화 된 형태로 설명합니다.\n",
    "\n",
    "* 예문 : \"what will the fat cat sit on\"\n",
    "\n",
    "예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해봅시다. 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측합니다. 훈련 과정에서는 'what will the fat cat'이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 'sit'을 예측하는 방식으로 훈련됩니다.\n",
    "\n",
    "훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 수치화하는 것입니다. 훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들을 다음과 같이 원-핫 인코딩 할 수 있습니다.\n",
    "\n",
    "```python\n",
    "\n",
    "what = [1, 0, 0, 0, 0, 0, 0]\n",
    "will = [0, 1, 0, 0, 0, 0, 0]\n",
    "the = [0, 0, 1, 0, 0, 0, 0]\n",
    "fat = [0, 0, 0, 1, 0, 0, 0]\n",
    "cat = [0, 0, 0, 0, 1, 0, 0]\n",
    "sit = [0, 0, 0, 0, 0, 1, 0]\n",
    "on = [0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "모든 단어가 단어 집합(vocabulary)의 크기인 7의 차원을 가지는 원-핫 벡터가 되었습니다. 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이 됩니다. **'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제**입니다.\n",
    "\n",
    "NNLM은 n-gram 언어 모델처럼 다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 **정해진 개수의 단어만을 참고**합니다. 이 개수를 n이라고 하고 n을 4라고 해봅시다. 이때, 언어 모델은 'what will the fat cat'라는 단어 시퀀스가 주어졌을 때, 다음 단어를 예측하기 위해 앞의 4개 단어 'will the fat cat'까지만 참고하고 그 앞 단어인 what은 무시합니다. 이 범위를 윈도우(window)라고 하기도 하는데, 여기서 윈도우의 크기인 n은 4입니다.\n",
    "\n",
    "![구조](./NNLM_구조.png)\n",
    "\n",
    "NNLM의 구조를 보겠습니다. NNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망입니다. 입력층(input layer)을 보면 앞에서 윈도우의 크기는 **4**로 정하였으므로 입력은 4개의 단어 'will, the, fat, cat'의 원-핫 벡터입니다. 출력층(output layer)을 보면 모델이 예측해야하는 정답에 해당되는 단어 sit의 원-핫 벡터는 모델이 예측한 값의 오차를 구하기 위해 레이블로서 사용됩니다. 그리고 오차로부터 손실 함수를 사용하여 인공 신경망이 학습을 하게 됩니다.\n",
    "\n",
    "내부 메커니즘을 따라가봅시다. 4개의 원-핫 벡터를 입력 받은 NNLM은 다음층인 **투사층(projection layer)**을 지나게 됩니다. 인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부르는데, 여기서 **투사층이라고 명명한 이 층은 일반 은닉층과 다르게 가중치 행렬과의 곱셈은 이루어지지만 활성화 함수가 존재하지 않습니다.**\n",
    "\n",
    "투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V × M 크기의 가중치 행렬과 곱해집니다. 여기서 V는 단어 집합의 크기를 의미합니다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면 가중치 행렬 W는 7 × 5 행렬이 됩니다.\n",
    "\n",
    "![calc](./calc.png)\n",
    "\n",
    "각 단어의 원-핫 벡터와 가중치 W 행렬의 곱이 어떻게 이루어지는지 보겠습니다. 위 그림에서는 각 원-핫 벡터를 $x$로 표기하였습니다. **원-핫 벡터의 특성으로 인해 i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것과(lookup) 동일합니다. 그래서 이 작업을 룩업 테이블(lookup table)이라고 합니다.**\n",
    "\n",
    "룩업 테이블 후에는 V차원을 가지는 원-핫 벡터는 이보다 더 차원이 작은 M차원의 벡터로 맵핑됩니다. 위 그림에서 단어 fat을 의미하는 원-핫 벡터를 $x_{\\text{fat}}$으로 표현했고, 테이블 룩업 과정을 거친 후의 단어 벡터는 $e_{\\text{fat}}$으로 표현했습니다. 이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경되는데 이 단어 벡터를 **임베딩 벡터(embedding vector)** 라고 합니다.\n",
    "\n",
    "![look](./lookup.png)\n",
    "\n",
    "각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경되고, 투사층에서 모든 임베딩 벡터들의 값은 연결됩니다(concatenate). 여기서 벡터의 연결 연산은 벡터들을 이어붙이는 것을 의미합니다. 가령, 5차원 벡터 4개를 연결한다는 의미는 20차원 벡터를 얻는다는 의미입니다. $x$를 각 단어의 원-핫 벡터, NNLM이 예측하고자 하는 단어가 문장에서 $t$번째 단어라고 하고, 윈도우의 크기를 $n$, 룩업 테이블을 의미하는 함수를 $lookup$, 세미콜론(;)을 연결 기호로 하였을 때 투사층을 식으로 표현하면 아래와 같습니다. (n-1까지인 이유는 n-gram처럼 n-1번째 수까지 참고하기 때문이다.)\n",
    "\n",
    "$$\n",
    "\\text{투사층}:p^{layer} = (lookup(x_{t-n});\\,...\\,;lookup(x_{t-2});\\,lookup(x_{t-1}))=(e_{t-n};\\,...\\,e_{t-2};\\,e_{t-1})\n",
    "$$\n",
    "\n",
    "일반적인 은닉층이 활성화 함수를 사용하는 비선형층(nonlinear layer)인 것과는 달리 **투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이라는 점이 다소 생소하지만, 이 다음은 다시 은닉층을 사용하는 일반적인 피드 포워드 신경망과 동일**합니다.\n",
    "\n",
    "![hidden](./hidden.png)\n",
    "\n",
    "투사층의 결과는 h의 크기를 가지는 은닉층을 지납니다. 일반적인 피드 포워드 신경망에서 은닉층을 지난다는 것은 은닉층의 입력은 가중치 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미입니다. 이때의 가중치와 편향을 $W_h$와 $b_h$이라고 하고, 은닉층의 활성화 함수를 하이퍼볼릭탄젠트 함수라고 하였을 때, 은닉층을 식으로 표현하면 아래와 같습니다.\n",
    "\n",
    "$$\n",
    "\\text{은닉층}:h^{layer}=tanh(W_hp^{layer}+b_h)\n",
    "$$\n",
    "\n",
    "\n",
    "![s2](./s2.png)\n",
    "\n",
    "은닉층의 출력은 V의 크기를 가지는 출력층으로 향합니다. 이 과정에서 다시 또 다른 가중치와 곱해지고 편향이 더해지면, **입력이었던 원-핫 벡터들과 동일하게 V차원의 벡터를 얻습니다.** 만약 입력 벡터의 차원이 7이었다면 해당 벡터도 동일한 차원 수를 가집니다. 출력층에서는 활성화 함수로*소프트맥스(softmax) 함수*를 사용하는데, **V차원의 벡터는 소프트맥스 함수를 지나면서 벡터의 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀝니다.** 이 벡터를 NNLM의 예측값이라는 의미에서 $\\hat{y}$ 라고 합시다. 이를 식으로 표현하면 아래와 같습니다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{출력층}:\\hat{y}=\\text{softmax}(W_yp^{layer}+b_y)\n",
    "$$\n",
    "\n",
    "벡터 \\hat{y}의 j번째 인덱스가 가진 0과 1사이의 값은 **j번째 단어가 다음 단어일 확률**을 나타냅니다. 그리고 **\\hat{y}는 실제값. 즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 합니다.** 실제값에 해당되는 다음 단어를 라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 NNLM는 손실 함수로 크로스 엔트로피(cross-entropy) 함수를 사용합니다. 해당 문제는 단어 집합의 모든 단어라는 V개의 선택지 중 정답인 'sit'을 예측해야하는 다중 클래스 분류 문제입니다. 그리고 역전파가 이루어지면 모든 가중치 행렬들이 학습되는데, 여기에는 **투사층에서의 가중치 행렬도 포함되므로 임베딩 벡터값 또한 학습**됩니다.\n",
    "\n",
    "이번 예제에서는 7개의 단어만 사용했지만, 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은 무엇일까요? NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 **결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게되는 것**에 있습니다. 이렇게 되면 훈련이 끝난 후 다음 단어를 예측 과정에서 (마치 앞서 언급한 A,B와 같은 예시처럼) **훈련 코퍼스에서 없던 단어 시퀀스라 하더라도 다음 단어를 선택할 수 있습니다.**\n",
    "\n",
    "단어 간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어서 딥 러닝 자연어 처리 모델에서는 필수적으로 사용되는 방법이 되었습니다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 4. NNLM의 이점과 한계\n",
    "---\n",
    "\n",
    "NNLM은 기존 n-gram 언어 모델의 한계를 개선했다.\n",
    "\n",
    "* 기존 모델에서의 개선점\n",
    "\n",
    "    NNLM은 단어를 표현하기 위해 임베딩 벡터를 사용하므로서 **단어의 유사도를 계산**할 수 있었다. 그리고 이를 통해 희소 문제(sparsity problem)를 해결했다.\n",
    "\n",
    "그러나 여전히 한계점도 있다. \n",
    "\n",
    "* 고정된 길이의 입력(Fixed-length input)\n",
    "\n",
    "    NNLM은 n-gram 언어 모델과 마찬가지로 **다음 단어를 예측하기 위해 모든 이전 단어를 참고하는 것이 아니라 정해진 n개의 단어만을 참고할 수 있다.** 이 한계를 극복할 수 있는 언어 모델이 있는데, 다음 챕터에서 배우게 될 RNN(Recurrent Neural Network)을 사용한 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
