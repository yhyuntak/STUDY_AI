{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf70f389",
   "metadata": {},
   "source": [
    "# 기울기 소실 Gradient Vanishing\n",
    "---\n",
    "\n",
    "만약 은닉층에 sigmoid를 사용하게 된다면 역전파 과정에서 **시그모이드의 미분에서의 곱셈**으로 인해 출력층 -> 입력층으로 갈수록 점점 gradient값이 0에 수렴하게 됨을 볼 수 있다. 이를 Gradient Vanishing이라고 한다. \n",
    "\n",
    "이 문제를 해결하려면, 은닉층에 sigmoid를 쓰지 않고, ReLU 계열의 activation function을 사용할 것을 추천한다. \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 기울기 폭주 Gradient Expanding \n",
    "---\n",
    "\n",
    "기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다. 이것은 뒤에서 배울 신경망인 RNN에서 유용하다고 한다. RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문이다. 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행한다.\n",
    "\n",
    "```python\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "\n",
    "```\n",
    "<br/><br/>\n",
    "\n",
    "# 가중치 초기화 Weight Initialization\n",
    "---\n",
    "\n",
    "가중치의 초기값에 따라 모델의 훈련 결과가 달라진다. 가중치 초기화만 잘 해줘도 기울기 소실과 같은 문제를 많이 완화시켜줄 수 있다고 한다.\n",
    "\n",
    "대표적인 초기화 방법은 2가지가 존재하며 각각 균등 분포(uniform distribution), 정규 분포(Normal distribution)으로 다.\n",
    "\n",
    "* **세이비어 초기화 Xavier Initialization**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
