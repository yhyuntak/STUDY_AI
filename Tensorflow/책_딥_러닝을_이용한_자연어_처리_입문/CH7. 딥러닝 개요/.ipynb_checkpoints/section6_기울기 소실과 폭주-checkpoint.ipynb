{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ead6e2",
   "metadata": {},
   "source": [
    "# 기울기 소실 (Gradient Vanishing)\n",
    "---\n",
    "\n",
    "만약 은닉층에 sigmoid를 사용하게 된다면 역전파 과정에서 **시그모이드의 미분에서의 곱셈**으로 인해 출력층 -> 입력층으로 갈수록 점점 gradient값이 0에 수렴하게 됨을 볼 수 있다. 이를 Gradient Vanishing이라고 한다. \n",
    "\n",
    "이 문제를 해결하려면, 은닉층에 sigmoid를 쓰지 않고, ReLU 계열의 activation function을 사용할 것을 추천한다. \n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 기울기 폭주 (Gradient Expanding)\n",
    "---\n",
    "\n",
    "기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다. 이것은 뒤에서 배울 신경망인 RNN에서 유용하다고 한다. RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문이다. 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행한다.\n",
    "\n",
    "```python\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)\n",
    "\n",
    "```\n",
    "<br/><br/>\n",
    "\n",
    "# 가중치 초기화 (Weight Initialization)\n",
    "---\n",
    "\n",
    "가중치의 초기값에 따라 모델의 훈련 결과가 달라진다. 가중치 초기화만 잘 해줘도 기울기 소실과 같은 문제를 많이 완화시켜줄 수 있다고 한다.\n",
    "\n",
    "대표적인 초기화 방법은 2가지가 존재하며 각각 균등 분포(uniform distribution), 정규 분포(Normal distribution)를 갖는다.\n",
    "\n",
    "\n",
    "### **세이비어 초기화 (Xavier Initialization)**\n",
    "\n",
    "* **균등 분포로 초기화**\n",
    "\n",
    "    l 층의 가중치를 초기화할때 l-1 층의 뉴런의 개수$n_{l-1}$와 l+1 층의 뉴런의 개수$n_{l+1}$을 사용해서 다음의 균등 분포로 가중치를 초기화 한다.\n",
    "\n",
    "    $$\n",
    "    W \\sim \\text{Uniform}(-\\sqrt{\\frac{6}{n_{l-1}+n_{l+1}}},+\\sqrt{\\frac{6}{n_{l-1}+n_{l+1}}})\n",
    "    $$\n",
    "\n",
    "\n",
    "* **정규 분포로 초기화 **\n",
    "\n",
    "    평균이 0이고 표준 편차 $\\sigma$가 다음을 만족하는 정규 분포로 가중치를 초기화한다.\n",
    "\n",
    "    $$\n",
    "    \\sigma = \\frac{2}{n_{l-1}+n_{l+1}}\n",
    "    $$\n",
    "\n",
    "\n",
    "세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞추어 특정 층이 너무 주목받거나 다른 층이 뒤쳐지는 것을 막는다. 그런데 세이비어 초기화는 **Sigmoid/tanh**와 같은 S자 형태인 activation과는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우 성능이 좋지 않다고 한다. \n",
    "\n",
    "\n",
    "### **He 초기화 (He Initialization)**\n",
    "\n",
    "** He 초기화는 ReLU 계열의 activation들과 함께 사용하는게 좋다고 한다. **\n",
    "\n",
    "* **균등 분포로 초기화**\n",
    "\n",
    "    l 층의 가중치를 초기화할때 세이비어 초기화와 달리 l-1 층의 뉴런의 개수$n_{l-1}$만 사용해서 다음의 균등 분포로 가중치를 초기화 한다.\n",
    "\n",
    "    $$\n",
    "    W \\sim \\text{Uniform}(-\\sqrt{\\frac{6}{n_{l-1}}},+\\sqrt{\\frac{6}{n_{l-1}}})\n",
    "    $$\n",
    "\n",
    "\n",
    "* **정규 분포로 초기화 **\n",
    "\n",
    "    평균이 0이고 표준 편차 $\\sigma$가 다음을 만족하는 정규 분포로 가중치를 초기화한다.\n",
    "\n",
    "    $$\n",
    "    \\sigma = \\frac{2}{n_{l-1}}\n",
    "    $$\n",
    "\n",
    "S자 형태의 activation은 세이비어가 효율적이고, ReLU계열은 He가 효율적이다. 근데 사실상 Sigmoid는 은닉층에 사용되지 않으므로, He + ReLU가 거의 정석적인 방법이 아닐까 싶다.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 배치 정규화 (Batch Normalization)\n",
    "---\n",
    "\n",
    "가중치를 He로 초기화함으로써 어느정도 기울기 소실/폭주를 완화할 수 있지만, 좀 더 확실히 예방하기 위해 배치 정규화(Batch Normalization)를 사용한다. 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하는 것이다.\n",
    "\n",
    "\n",
    "### 내부 공변량 변화 (Internal Covariate Shift)\n",
    "\n",
    "배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있다. 내부 공변량 변화란 **학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상**을 뜻한다. 역전파로 l-1 층의 가중치 값이 바뀌게 되면, feedforward로 l 층에 전달되는 입력 데이터의 분포가 l-1 층이 이전에 학습했던 시점의 분포와 차이가 발생한다.(책에선 이렇게 쓴 것 같은데 확실하진 않다.) **배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문**이라고 주장한다. \n",
    "\n",
    "그러나 사실 BN(Batch Normalization)은 ICS를 해결해 주는 게 아니었다..!! [논문 How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)을 보면 **BN의 역할은 손실 함수의 평면과 기울기를 smoothing 해주는 효과가 있다고 한다!** 놀랍구먼.. 논문을 읽어도 되고 논문 저자가 쓴 [게시글](https://gradientscience.org/batchnorm/)봐도 좋다.\n",
    "\n",
    "### 배치 정규화 (Batch Normalization)\n",
    "\n",
    "배치 정규화(Batch Normalization)는 각 층에서 활성화 함수를 통과하기 전에 수행된다. 정규화를 요약하면 다음과 같다고 한다. \n",
    "> 입력에 대해 평균을 0으로 만들고, 정규화를 한다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행한다. 이때 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 한다.수식은 아래 그림처럼 사용한다. \n",
    "\n",
    "![BN eqn](./images/BN_eqn.png)\n",
    "\n",
    "스케일과 시프트에 대한 내용이 정확히 이해가 가진 않는다. 후에 추가적으로 학습하자. \n",
    "\n",
    "앞서 배치 정규화의 식에서 평균과 분산들이 어떤 방식으로 쪼개져서 구해지는지 조금 이해가 안갔었는데 아래 그림을 보니 이해가 간다.\n",
    "\n",
    "![BN picture](./images/BN_picture.png)\n",
    "\n",
    "**각각의 feature들에 대해서 배치의 평균과 분산을 구하는 것**이었다~! 이해 완료!!\n",
    "\n",
    "이렇게 배치 정규화를 사용하면 몇가지 장점이 있다.\n",
    "* 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선된다.\n",
    "* 가중치 초기화에 훨씬 덜 민감해진다.\n",
    "* 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.\n",
    "* 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 낸다. 다시 말해, 마치 드롭아웃과 비슷한 효과를 낸다. 물론, 드롭 아웃과 함께 사용하는 것이 좋다고 한다.\n",
    "\n",
    "하지만 단점도 있다. \n",
    "* 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려진다. 그래서 **서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요하다.**\n",
    "\n",
    "한계점도 존재한다.\n",
    "* 미니 배치 크기에 의존적이다.\n",
    "\n",
    "    극단적으로 배치의 크기가 1이라고 한다면, 분산은 위 식에 의해서 0이 된다. 이처럼 작은 미니 배치에선 배치 정규화가 극단적으로 작용해 학습시에 악영향을 끼칠 수 있다. 따라서 어느정도 크기가 있는 미니 배치에서 배치 정규화를 사용해야한다. \n",
    "    \n",
    "* **RNN에 적용하기 어렵다.**\n",
    "\n",
    "    RNN은 각 시점마다 다른 통계치를 갖기 때문에, 정규화를 적용하는 것이 힘들다고 한다. 그래서 보통 아래에서 소개할 층 정규화를 사용하는 것 같다.\n",
    "    \n",
    "<br/><br/>\n",
    "\n",
    "# 층 정규화 (Layer Normalization)\n",
    "---\n",
    "\n",
    "![Layer picture](./images/Layer_pic.png)\n",
    "\n",
    "\n",
    "위 그림은 층 정규화의 모습을 보여준다. 층 정규화는 feature별로 배치의 평균과 분산 구하는 배치 정규화와 달리, 각각의 배치의 평균과 분산을 구한다. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
