{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8cef3e",
   "metadata": {},
   "source": [
    "# 1. 손실 함수(Loss function)\n",
    "---\n",
    "\n",
    "손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수이다. 회귀에서는 평균 제곱 오차(MSE), 분류 문제에서는 크로스 엔트로피(CE)를 주로 손실 함수로 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec406943",
   "metadata": {},
   "source": [
    "## 1) MSE\n",
    "평균 제곱 오차는 연속형 변수를 회귀 예측할 때 사용된다.\n",
    "코드로 작성할 땐 다음과 같이 사용한다.\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "그러나 NLP에선 주로 분류문제로 판단하기 때문에, 아래에서 정리할 CE를 주로 쓴다.\n",
    "\n",
    "## 2) Binary CE\n",
    "\n",
    "로지스틱 회귀처럼 이진 분류시 사용되는 크로스 엔트로피 함수다. 사용법은 다음과 같다.\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## 3) Categorical CE\n",
    "\n",
    "다중 클래스 분류시 사용되는 크로스 엔트로피 함수다. 사용법은 다음과 같다.\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "위 코드는 원-핫 인코딩이 된 경우에 사용하는데, 그렇지 않고 정수 값을 가진 레이블을 사용할 경우엔 아래와 같이 sparse_categorical CE를 쓴다.\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam',metrics=['acc'])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "그 외에 손실 함수들에 대한 정보는 텐서플로우 공식 문서를 확인하자. [링크](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2c787",
   "metadata": {},
   "source": [
    "# 2. 배치 크기(Batch Size)에 따른 경사 하강법\n",
    "---\n",
    "\n",
    "배치 크기에 따라 학습법이 다르다. 배치 경사 하강법(모든 데이터를 한번에 한 배치에 사용), 배치 크기가 1인 SGD (배치 크기가 1로 랜덤하게 샘플 데이터를 하나씩 뽑아서 학습)이 있는데, 전자는 전체 데이터를 한번에 사용하기 때문에 local minima에 빠질 위험이 적지만, 많은 메모리 사용과 parameter 업데이트시에 시간이 오래걸린다. 후자는 업데이트가 빠르지만 local minima에 빠질 위험이 있다. \n",
    "\n",
    "따라서 두 방법을 적절히 섞은 미니배치 방법을 주로 사용한다. 아래는 배치 사이즈를 128로 두고 모델을 학습하는 코드이다.\n",
    "\n",
    "```python\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e65c06",
   "metadata": {},
   "source": [
    "# 3. 옵티마이저\n",
    "---\n",
    "\n",
    "최적화를 하는 과정에서 Momentum, Adagrad, RMSprop, Adam 등 다양한 방법론이 나왔다. 각자 특징들이 있는데, 가장 단점들을 많이 보완한 것이 Adam인 것 같다. 물론 지금 시점에선 더 뛰어난 최적화방법이 있겠지만, 깊이 공부할 것이 아니기에 주로 Adam을 사용하자. 위 4가지 방법에 대한 코딩만 간단히 적고 가도록 하자.\n",
    "\n",
    "* Momentum <br/>\n",
    "\n",
    "```python\n",
    "\n",
    "tf.keras.optimizers.SGD(lr=0.01,momentum=0.9)\n",
    "\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Adagrad <br/>\n",
    "\n",
    "```python\n",
    "\n",
    "tf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
    "\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "* RMSprop <br/>\n",
    "\n",
    "```python\n",
    "\n",
    "tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Adam <br/>\n",
    "\n",
    "```python\n",
    "\n",
    "tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "사용방법은 사실 제일 위 손실함수를 이야기할 때 이미 썼었다. \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "```\n",
    "\n",
    "그러나 위 방식처럼 적으면 adam의 하이퍼파라미터들을 건드릴 수 없다. 하이퍼파라미터를 조정하고 싶다면 다음과 같이 객체를 선언해서 사용하면 된다.\n",
    "\n",
    "```python\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "```\n",
    "\n",
    "더 많은 옵티마이저들은 [링크](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)를 확인하자.\n",
    "개념은 [링크](https://www.jeremyjordan.me/gradient-descent/)를 참고하자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
