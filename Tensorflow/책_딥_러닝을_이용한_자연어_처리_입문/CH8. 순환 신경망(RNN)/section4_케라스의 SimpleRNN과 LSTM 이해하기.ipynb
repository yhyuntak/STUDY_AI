{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ece48d",
   "metadata": {},
   "source": [
    "# 1. 임의의 입력 생성하기\n",
    "---\n",
    "\n",
    "이번 세션은 임의의 예시를 통해 RNN과 LSTM을 이해해보려는 것같다.\n",
    "\n",
    "먼저 RNN과 LSTM을 테스트하기 위한 임의의 입력을 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2a35cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array([[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]],dtype=np.float32)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22c939",
   "metadata": {},
   "source": [
    "위 입력은 **단어 벡터의 차원은 5**이고, **문장의 길이가 4**인 경우를 가정한 임베딩 벡터이다. 실수로 이루어져 함축적인 의미를 갖고 있을 것이기 때문. 다시 말해 4번의 시점(timesteps)이 존재하고, 각 시점마다 5차원의 단어 벡터가 입력으로 사용된다. 그런데 앞서 RNN은 2D 텐서가 아니라 3D 텐서를 입력을 받는다고 언급한 바 있다. 즉, 위에서 만든 2D 텐서를 3D 텐서로 변경한다. 이는 배치 크기 1을 추가해주므로서 해결한다. 방법은 여러가지가 있는데, 나는 책에 예제랑 달리 np.newaxis를 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10131168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X[np.newaxis,:,:]\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfae78",
   "metadata": {},
   "source": [
    "이로써 (batch_size, timesteps, input_dim)에 해당되는 (1, 4, 5)의 크기를 가지는 3D 텐서가 생성되었다.\n",
    "\n",
    "<br/><br/> \n",
    "# 2. SimpleRNN 이해하기\n",
    "---\n",
    "\n",
    "은닉 상태의 크기가 3인 단일 SimpleRNN을 설계해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4112ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[-0.7749774  -0.8574275  -0.43291962]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN,LSTM,Bidirectional\n",
    "\n",
    "rnn = SimpleRNN(3)\n",
    "hidden_state = rnn(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569c2b2",
   "metadata": {},
   "source": [
    "은닉 상태의 크기를 3으로 지정했음을 주목하자. 따라서 (1, 3) 크기의 텐서가 출력되는데, 이것은 **마지막 시점의 은닉 상태**이다. 왜냐하면 인자 return_sequences가 default로 False로 지정되어 있기 때문이다. 이것이 False로 지정되어있으면, 마지막 시점의 은닉 상태만 출력된다. \n",
    "\n",
    "이번에는 return_sequences를 True로 지정하여 모든 시점의 은닉 상태를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5fb33f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[[-0.9998527  -0.9372029  -0.8851823 ]\n",
      "  [-0.99983495  0.23072506  0.49233142]\n",
      "  [-0.9894148   0.11405627  0.28647438]\n",
      "  [-0.99644697 -0.9352896   0.8385667 ]]], shape: (1, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(3,return_sequences=True)\n",
    "hidden_state = rnn(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59a791",
   "metadata": {},
   "source": [
    "(1, 4, 3) 크기의 텐서가 출력되었다. 앞서 입력 데이터는 (1, 4, 5)의 크기를 가지는 3D 텐서였고, 그 중 시점(timesteps)에 해당하는 값이 4이므로 모든 시점에 대해서 은닉 상태의 값을 출력하여 (1, 4, 3) 크기의 텐서를 출력하는 것이다.\n",
    "\n",
    "SimpleRNN에는 return_state라는 인자도 있다. 이것이 True일 경우에는 return_sequences의 True/False 여부와 상관없이, 마지막 시점의 은닉 상태를 출력해준다. 가령, return_sequences가 True이면서, return_state를 True로 할 경우 SimpleRNN은 두 개의 출력을 리턴한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98d3da35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[ 0.9988733  -0.9970756  -0.02535443]\n",
      "  [ 0.9684544  -0.9535599  -0.25670364]\n",
      "  [ 0.9228492  -0.8421958   0.7329244 ]\n",
      "  [ 0.9778378  -0.660664   -0.98326814]]], shape: (1, 4, 3)\n",
      "last hidden state : [[ 0.9778378  -0.660664   -0.98326814]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_state = rnn(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed292e",
   "metadata": {},
   "source": [
    "첫번째 출력은 return_sequences=True로 인한 출력으로 모든 시점의 은닉 상태이다. 두번째 출력은 return_state=True로 인한 출력으로 마지막 시점의 은닉 상태이다. 실제로 출력을 보면 **모든 시점의 은닉 상태인 (1, 4, 3) 텐서의 마지막 벡터값이 return_state=True로 인해 출력된 벡터값과 일치하는 것**을 볼 수 있다. (둘 다 [-0.5144398 -0.5037417 0.96605766])\n",
    "\n",
    "그렇다면 return_sequences는 False인데, retun_state가 True인 경우는 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c99258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[ 0.03025864  0.21237555 -0.93383026]], shape: (1, 3)\n",
      "last hidden state : [[ 0.03025864  0.21237555 -0.93383026]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(3, return_sequences=False, return_state=True)\n",
    "hidden_state, last_state = rnn(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f2c2c",
   "metadata": {},
   "source": [
    "두 개의 출력 모두 마지막 시점의 은닉 상태를 출력하게 된다.\n",
    "\n",
    "<br/><br/>\n",
    "# 3. LSTM 이해하기\n",
    "---\n",
    "\n",
    "실제로 SimpleRNN이 사용되는 경우는 거의 없다. 이보다는 LSTM이나 GRU을 주로 사용하는데, 이번에는 임의의 입력에 대해서 LSTM을 사용할 경우를 보자. 우선 return_sequences를 False로 두고, return_state가 True인 경우를 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e7ad4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden state : [[-0.0480977  -0.03767991 -0.4076246 ]], shape: (1, 3)\n",
      "last hidden state : [[-0.0480977  -0.03767991 -0.4076246 ]], shape: (1, 3)\n",
      "last cell state : [[-0.22368245 -0.1395814  -1.0070326 ]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(3, return_sequences=False, return_state=True)\n",
    "hidden_state, last_state, last_cell_state = lstm(train_X)\n",
    "\n",
    "print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))\n",
    "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768cb371",
   "metadata": {},
   "source": [
    "LSTM이 SimpleRNN과 다른 점은 **return_state를 True로 둔 경우에는 마지막 시점의 은닉 상태뿐만 아니라 셀 상태까지 반환한다.** 그래서 다음의 세 개의 결과를 반환해준다.\n",
    "\n",
    "1. return_sequences가 False이므로 마지막 시점의 은닉 상태 출력 \n",
    "2. return_state = True에 의해 마지막 시점의 은닉 상태 출력\n",
    "3. return_state = True에 의해 마지막 시점의 셀 상태 출력\n",
    "\n",
    "이번에는 return_sequences를 True로 바꾼 후, 결과를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b6bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[-0.08173853 -0.01507639 -0.01830817]\n",
      "  [-0.03099248 -0.01895461 -0.04041291]\n",
      "  [-0.05361801 -0.03426022 -0.13037403]\n",
      "  [-0.20434079 -0.02855435 -0.21408609]]], shape: (1, 4, 3)\n",
      "last hidden state : [[-0.20434079 -0.02855435 -0.21408609]], shape: (1, 3)\n",
      "last cell state : [[-0.4921951  -0.08731357 -0.77757996]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(3, return_sequences=True, return_state=True)\n",
    "hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\n",
    "print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e6987",
   "metadata": {},
   "source": [
    "예상할 수 있듯이, 모든 시점의 은닉 상태들이 출력됨을 볼 수 있다.\n",
    "\n",
    "<br/><br/>\n",
    "# 3. Bidirectional(LSTM) 이해하기\n",
    "---\n",
    "\n",
    "이번엔 양방향 LSTM의 출력값을 확인해보자. return_sequences가 True인 경우와 False인 경우에 대해서 은닉 상태의 값이 어떻게 바뀌는지 직접 비교하기 위해, 이번에는 가중치들을 직접 초기화해서 출력되는 은닉 상태의 값을 고정시키겠다.\n",
    "\n",
    "코드에 들어가기에 앞서서, LSTM에는 {kernel/recurrent/bias}_initializer 라는 인자들이 있다. 각각에 대해 먼저 설명하고 넘어가겠다.\n",
    "\n",
    "* kernel_initializer \t\n",
    "    \n",
    "    입력 값과 곱해지는 가중치 행렬들의 초기 값을 어떻게 설정할 것인지 묻는 것. default는 glorot_uniform이다. <br/>\n",
    "    원문 : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. Default: glorot_uniform.\n",
    "    \n",
    "* recurrent_initializer  \n",
    "\n",
    "    은닉 상태 벡터들과 곱해지는 가중치 행렬들의 초기 값을 어떻게 설정할 것인지 묻는 것. default는 orthogonal이다. <br/> \n",
    "    원문 : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. Default: orthogonal.\n",
    "\n",
    "* bias_initializer \n",
    "\n",
    "    편향 벡터들의 초기 값을 묻는 것. default는 0 벡터다.<br/>\n",
    "    원문 : Initializer for the bias vector. Default: zeros. \n",
    "    \n",
    "RNN,LSTM 등은 가중치들을 **모든 시점에서 공유**한다. 따라서 초기 값을 어떻게 설정할 것인지 묻는 것이다. 이렇게 가중치를 공유함으로써 장점을 얻는다. \n",
    "\n",
    "* 학습에 필요한 가중치의 수를 줄일 수 있다.\n",
    "* 데이터별 시간의 길이 T에 유연하다. \n",
    "\n",
    "두번째 장점에 대해선 상상을 해보면 이해가 되는 것 같다. NLP란 결국 많은 문장들을 받아들이게 될 것인데 특히 한국말의 경우엔 가중치 공유라는게 더 큰 장점으로 발휘될 것 같다. 예를 들어\n",
    "\n",
    "\"나는 머리를 자르러 []에 간다.\" 와 \"나는 []에 가서 머리를 자른다.\"\n",
    "\n",
    "를 보자. 두 문장은 사실 의미 자체는 같다고 볼 수 있다. 단지 시점의 차이만 있을 뿐이다. 빈칸에는 미용실을 예측하게 될 것인데, 시점에 들어가는 단어 표현이 다르다고 다르게 예측되지 않는다. 즉, 문장의 의미 자체를 학습하는게 포인트인 것이다. 따라서 하나의 가중치로 공유를 해도 학습이 되야만 하는 것이다. 이것은 극히 주관적인 판단이긴 하지만.. 아마 이런 이유때문에 가중치 공유를 하는게 아닐까 싶다.\n",
    "\n",
    "다시 본론으로 들어와서.. 은닉 상태들의 출력 값을 고정시키기 위해서 initializers로 가중치를 초기화해 고정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "766ea7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[0.6303139  0.6303139  0.6303139  0.70387346 0.70387346 0.70387346]], shape: (1, 6)\n",
      "forward state : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)\n",
      "backward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# initializers.Constant는 value 값으로 모든 값을 초기화 시켜주는 모듈이다. \n",
    "\n",
    "k_init = tf.keras.initializers.Constant(value=0.1)\n",
    "b_init = tf.keras.initializers.Constant(value=0)\n",
    "r_init = tf.keras.initializers.Constant(value=0.1)\n",
    "\n",
    "bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52bc7a4",
   "metadata": {},
   "source": [
    "이번에는 무려 5개의 값을 반환한다. return_state가 True인 경우에는 정방향 LSTM의 은닉 상태와 셀 상태, 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환하기 때문이다. 다만, 셀 상태는 각각 forward_c와 backward_c에 저장만 하고 출력하지 않았다. \n",
    "\n",
    "첫번째 출력값의 크기가 (1, 6)인 것에 주목하자. 이것은 return_sequences가 False인 경우, **정방향 LSTM의 마지막 시점의 은닉 상태**와 **역방향 LSTM의 첫번째 시점의 은닉 상태**가 연결된 채 반환되기 때문이다. 아래 그림은 이 경우를 보여주며, $y_t$가 hidden_states를 의미한다고 보면 된다.\n",
    "\n",
    "![그림 1](./images/section4/그림_1.png)\n",
    "\n",
    "마찬가지로 return_state가 True인 경우에 반환한 은닉 상태의 값인 forward_h와 backward_h는 각각 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태값이다. 이 두개가 붙어서 hidden_states에 저장된다고 생각하자.\n",
    "\n",
    "그럼 return_sequences를 True로 할 경우, 출력이 어떻게 바뀌는지 확인하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3caadd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states : [[[0.35906476 0.35906476 0.35906476 0.70387346 0.70387346 0.70387346]\n",
      "  [0.5511133  0.5511133  0.5511133  0.5886358  0.5886358  0.5886358 ]\n",
      "  [0.5911575  0.5911575  0.5911575  0.39516988 0.39516988 0.39516988]\n",
      "  [0.6303139  0.6303139  0.6303139  0.2194224  0.2194224  0.2194224 ]]], shape: (1, 4, 6)\n",
      "forward state : [[0.6303139 0.6303139 0.6303139]], shape: (1, 3)\n",
      "backward state : [[0.70387346 0.70387346 0.70387346]], shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\n",
    "                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n",
    "hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n",
    "\n",
    "print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n",
    "print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n",
    "print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d5bb0",
   "metadata": {},
   "source": [
    "hidden states의 출력값에서는 이제 모든 시점의 은닉 상태가 출력된다. 역방향 LSTM의 첫번째 시점의 은닉 상태는 더 이상 정방향 LSTM의 마지막 시점의 은닉 상태와 연결되는 것이 아니라 **정방향 LSTM의 첫번째 시점의 은닉 상태와 연결**된다. 아래 그림은 return_sequences=True의 hidden_states의 엮이는 모습을 보여준다. \n",
    "\n",
    "![그림 2](./images/section4/그림_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
