{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae794e2e",
   "metadata": {},
   "source": [
    "CH7에서 배웠던 NLMM은 입력 길이가 고정되어 있어 자연어 처리를 하기엔 한계가 있었다. 따라서 다양한 길이의 입력 시퀀스를 처리할 수 있는 순환 신경망(RNN)이 등장하게 되었다.\n",
    "\n",
    "# 1. 순환 신경망(Recurrent Neural Network, RNN)\n",
    "---\n",
    "\n",
    "RNN(Recurrent Neural Network)은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델이다.. 시퀀스 단위가 뭔지 아직 잘 모르겠다. 어쨌든, 번역기를 예로 들면 **입력은 번역하고자 하는 단어의 시퀀스인 문장**이다. **출력에 해당되는 번역된 문장 또한 단어의 시퀀스**이다. 이와 같이 **시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델**이라고 한다. 그 중 RNN은 가장 기본적인 인공 신경망 시퀀스 모델이다.\n",
    "\n",
    "RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, **다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징**을 갖고있다.\n",
    "\n",
    "![그림 1](./images/section1/rnn_image1_ver2.png)\n",
    "\n",
    "위 그림은 RNN의 큰 틀을 보여준다. 현재 시점을 t 라고 한다. RNN에서 **은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)**이라고 한다. 이 셀은 **이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 메모리 셀 또는 RNN 셀**이라고 표현한다. \n",
    "\n",
    "은닉층의 메모리 셀은 각각의 시점(time step) t 에서 바로 이전 시점 t-1 에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용한다. 이것은 **현재 시점 t에서의 메모리 셀이 갖고있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미**한다.\n",
    "\n",
    "**메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값을 은닉 상태(hidden state)** 라고 한다. 다시 말해 t 시점의 메모리 셀은 t 시점의 은닉 상태 계산을 위해 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 입력값으로 사용합니다. 그리고 또 t+1 시점에게 은닉 상태를 입력으로 보낼 것이다.  \n",
    "\n",
    "![그림 2](./images/section1/RNN_펼친_것.png)\n",
    "\n",
    "위 그림은 RNN을 시점의 흐름대로 보기 위해 펼친 것이다. 시점은 1,2,...,t 까지 전개되어 있다. RNN에서는 뉴런이라는 단위보다는 **입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현**을 주로 사용한다. 위의 그림에서 회색과 초록색으로 표현한 각 네모들은 기본적으로 벡터 단위를 가정하고 있다. 뉴런 단위를 사용하는 피드 포워드 신경망과의 차이를 비교하기 위해서 RNN을 뉴런 단위로 시각화하면 다음 그림과 같다.\n",
    "\n",
    "\n",
    "![그림 3](./images/section1/그림_3.png)\n",
    "\n",
    "입력 벡터의 차원은 4이고, 은닉 상태와 출력은 차원이 2인 벡터로 이루어져 있다. t=1일 때에서 t=2일 때로 넘어갈때, 각각의 은닉 상태들은 각자 위치에 맞게 전달됨을 볼 수 있다. \n",
    "\n",
    "\n",
    "![그림 4](./images/section1/그림_4.png)\n",
    "\n",
    "RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용할 수 있다. 위 그림은 **입력과 출력의 길이에 따라서 달라지는 RNN의 다양한 형태**를 보여준다. 위 구조가 자연어 처리에서 어떻게 사용될 수 있는지 예를 들어보자. RNN 셀의 각 시점의 입, 출력의 단위는 사용자가 정의하기 나름이지만 가장 보편적인 단위는 **'단어 벡터'**이다.\n",
    "\n",
    "예를 들어 하나의 입력에 대해서 여러개의 출력을 의미하는 일 대 다(one-to-many) 구조의 모델은 **하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning) 작업**에 사용할 수 있다. **사진의 제목은 단어들의 나열이므로 시퀀스 출력**이다. 아마 입력으로 이미지가 하나만 들어가면, 여러 출력에서 제목을 순서대로 보여주는 것 같다!! 이 개념을 토대로 많은 논문들이 쓰였겠구만.. \n",
    "\n",
    "![그림 5](./images/section1/그림_5.png)\n",
    "\n",
    "또한 단어 시퀀스에 대해서 하나의 출력을 하는 다 대 일(many-to-one) 구조의 모델은 **입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류(sentiment classification), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류(spam detection) 등**에 사용할 수 있다. 위 그림은 RNN으로 스팸 메일을 분류할 때의 아키텍처를 보여준다. 이러한 예제들은 나중에 Chapter 10의 'RNN을 이용한 텍스트 분류'에서 배울 예정이다. 재밌겠다!!\n",
    "\n",
    "\n",
    "![그림 6](./images/section1/그림_6.png)\n",
    "\n",
    "다 대 다(many-to-many) 구조의 모델의 경우에는 **사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 또는 Chapter 12의 '태깅 작업' 에서 배우는 개체명 인식이나 품사 태깅과 같은 작업이 속한다.** 위 그림은 개체명 인식을 수행할 때의 RNN 아키텍처를 보여준다. 번역기 작업을 생각하면 다대다가 맞는 것 같다. 한글을 영어로 매칭하며 번역해야하니까.. \n",
    "\n",
    "이제 RNN에 대한 수식을 정의해보자. 먼저 시점 t 구간을 떼어내서 봐보자.\n",
    "\n",
    "\n",
    "![그림 7](./images/section1/그림_7.png)\n",
    "\n",
    "$h_t$는 시점 t에서의 은닉 상태값이다. 은닉층의 메모리 셀은 $h_t$를 계산하기 위해 $W_h,\\,W_x$를 사용한다. bias $b$까지 사용하면, $h_t$는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "h_t = tanh(W_xx_t+W_hh_{t-1}+b)\n",
    "$$\n",
    "\n",
    "은닉 상태를 구할 땐 tanh를 주로 사용한다고 한다. 그리고 출력층으로 갈 땐, 활성화 함수 $f$를 선택하고, 가중치 $W_y$와 $h_t$를 사용해 다음과 같이 쓸 수 있다.\n",
    "\n",
    "$$ \n",
    "y_t = f(W_yh_t+b)\n",
    "$$\n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 2. 케라스(Keras)로 RNN 구현하기\n",
    "---\n",
    "\n",
    "코드를 보기 전에, 먼저 입력으로 사용되는 데이터의 구조를 보자.\n",
    "\n",
    "![그림 8](./images/section1/그림_8.png)\n",
    "\n",
    "위 그림에서 batch_size는 말 그대로 배치의 크기를 이야기한다. 그리고 timesteps은 시점의 수라고 생각하면 된다. Input_dim은 각 시점에서 사용되는 단어 표현의 차원 즉, 단어의 수? 라고 생각하고 넘어가자.\n",
    "\n",
    "RNN의 은닉층에 해당하는 코드는 다음과 같이 케라스에 SimpleRNN으로 내장되어있다.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "```\n",
    "\n",
    "![그림 9](./images/section1/그림_9.png)\n",
    "\n",
    "그러나 이것은 출력층을 포함하지 않고 위 그림처럼 단순히 입력을 받아 은닉 상태만을 반환하는 구조로 되어있다.\n",
    "\n",
    "![그림 10](./images/section1/그림_10.png)\n",
    "\n",
    "이 RNN 층은 return_sequences 매개 변수를 조정함으로써 위와 같이 두 가지 종류의 출력을 내보낼 수 있다. \n",
    "\n",
    "1. **return_sequences = False** : 메모리 셀의 **최종 시점의 은닉 상태만**을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서를 리턴(좌측 그림)\n",
    "2. **return_sequences = True** : 메모리 셀의 각 시점(time step)의 은닉 상태값들을 모아서 **전체 시퀀스를 리턴**하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴(우측 그림)\n",
    "\n",
    "최종 시점의 은닉 상태만을 리턴한다면 **다 대 일**문제를 풀 수 있고, 전체 시퀀스를 리턴한다면 **다 대 다**문제를 풀 수 있다.\n",
    "\n",
    "아래 예제는 배치 크기 = 8, timesteps = 2, 단어 표현의 수 = 10 인 입력이 들어갔을 때, 다 대 일 출력을 보여준다.\n",
    "은닉 상태의 차원을 3으로 지정했으니, 최종 시점의 은닉 상태가 (배치 크기,은닉 상태의 차원) 형태로 출력되는 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "382b59f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_21 (SimpleRNN)   (8, 3)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "# hidden_units, batch_input_shape=(batch,timesteps, input_dim)\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=False)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86495c4b",
   "metadata": {},
   "source": [
    "만약 다 대 다 출력을 원하면, return_sequences = True로 변경하면 되고, \n",
    "같은 예제에서 모든 시퀀스가 출력되므로 (배치 크기,timesteps,은닉 상태의 차원) 형태로 출력되는 것 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76ae22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_22 (SimpleRNN)   (8, 2, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "# hidden_units, batch_input_shape=(batch,timesteps, input_dim)\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08f29f",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "# 3. 파이썬으로 RNN 구현하기\n",
    "---\n",
    "\n",
    "책에서 아주 간단하게 RNN의 다 대 다 순전파를 예시로 보여준다. \n",
    "각 시점에서의 은닉 상태 값들은 벡터형식이고 최종 출력을 은닉 상태들의 stack된 것을 본다고 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da0c837d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 은닉 상태 : [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "가중치 Wx의 크기(shape) : (8, 4)\n",
      "가중치 Wh의 크기(shape) : (8, 8)\n",
      "편향의 크기(shape) : (8,)\n",
      "모든 시점의 은닉 상태 :\n",
      "[[0.80642975 0.88029468 0.93213641 0.82337915 0.82793623 0.94374638\n",
      "  0.94668245 0.76011434]\n",
      " [0.99999768 0.99997938 0.99974882 0.99986779 0.9999422  0.99988892\n",
      "  0.99998973 0.99869091]\n",
      " [0.99999894 0.99999534 0.99972703 0.9999684  0.9998821  0.9998548\n",
      "  0.99999552 0.99952883]\n",
      " [0.99999872 0.99999356 0.99963686 0.99993477 0.99995765 0.99991424\n",
      "  0.99999612 0.99944261]\n",
      " [0.99999833 0.99999261 0.99933366 0.99993711 0.99991739 0.9998435\n",
      "  0.9999938  0.99949494]\n",
      " [0.99999476 0.99998691 0.99855298 0.99986567 0.99959793 0.99935539\n",
      "  0.99998232 0.99924644]\n",
      " [0.99999878 0.99999115 0.99948412 0.99994353 0.99987801 0.99974325\n",
      "  0.99998991 0.99946202]\n",
      " [0.99999909 0.99999536 0.999734   0.99997327 0.9998606  0.99982943\n",
      "  0.99999485 0.99955665]\n",
      " [0.9999964  0.9999903  0.99932721 0.99987485 0.99985025 0.99975269\n",
      "  0.99999197 0.99921282]\n",
      " [0.99999927 0.99999646 0.99986386 0.99997045 0.99995531 0.99994216\n",
      "  0.99999787 0.99951559]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 10개 순서대로 값들이 들어간다.\n",
    "input_dim = 4 # 단어 표현의 수가 4다. 즉, 한 시점당 단어가 4개 들어간다는 것과 비슷한 느낌.\n",
    "hidden_units = 8 # 각 시점에서의 은닉 상태 벡터의 차원을 얘기한다.\n",
    "\n",
    "# 입력에 해당되는 2D 텐서\n",
    "inputs = np.random.random((timesteps, input_dim))\n",
    "\n",
    "# 초기 은닉 상태는 0(벡터)로 초기화\n",
    "hidden_state_t = np.zeros((hidden_units,)) \n",
    "\n",
    "print('초기 은닉 상태 :',hidden_state_t)\n",
    "\n",
    "Wx = np.random.random((hidden_units, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_units, hidden_units)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_units)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
    "\n",
    "print('가중치 Wx의 크기(shape) :',np.shape(Wx))\n",
    "print('가중치 Wh의 크기(shape) :',np.shape(Wh))\n",
    "print('편향의 크기(shape) :',np.shape(b))\n",
    "\n",
    "# 다 대 다의 은닉 상태들을 묶어서 표현.\n",
    "total_hidden_states = []\n",
    "\n",
    "# 각 시점 별 입력값.\n",
    "for input_t in inputs:\n",
    "\n",
    "    # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "    output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\n",
    "\n",
    "    # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep t, output_dim)\n",
    "    # 각 시점의 은닉 상태의 값을 계속해서 누적\n",
    "    total_hidden_states.append(list(output_t))\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.array(total_hidden_states) \n",
    "\n",
    "# (timesteps, output_dim)\n",
    "print('모든 시점의 은닉 상태 :')\n",
    "print(total_hidden_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab46bd",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "4. 깊은 순환 신경망(Deep Recurrent Neural Network)\n",
    "---\n",
    "\n",
    "![그림 11](./images/section1/그림_11.png)\n",
    "\n",
    "RNN은 점점 깊어질수록 위 그림처럼 은닉층이 위에 형성되는 느낌인 것 같다. 오른쪽으로 쌓여가는 것은 시점의 수에 따라 표현되는 것이고, 깊어진다는 개념은 위로 쌓여가는 것이다. 은닉층을 2개 사용하는 간단한 다 대 다 RNN 코드는 다음과 같다.\n",
    "\n",
    "```python\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, input_length=10, input_dim=5, return_sequences=True))\n",
    "model.add(SimpleRNN(hidden_units, return_sequences=True))\n",
    "\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "5. 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
    "---\n",
    "\n",
    "기존의 RNN은 시점 t에서의 출력값을 예측할 때, 이전 시점의 입력에만 의존했다. 그러나 양방향 RNN은 이전 시점의 입력뿐만 아니라, 이후 시점의 입력 또한 예측에 기여할 수 있다는 아이디어에 기반한 네트워크이다. 빈칸 채우기 문제에 비유해보자.\n",
    "\n",
    "```python\n",
    "운동을 열심히 하는 것은 [        ]을 늘리는데 효과적이다.\n",
    "\n",
    "1) 근육\n",
    "2) 지방\n",
    "3) 스트레스\n",
    "```\n",
    "'운동을 열심히 하는 것은 [ ]을 늘리는데 효과적이다.' 라는 문장에서 문맥 상으로 정답은 '근육'이다. 위의 빈 칸 채우기 문제를 풀 때 이전에 나온 단어들만으로 빈 칸을 채우려고 시도해보면 정보가 부족하다. '운동을 열심히 하는 것은' 까지만 주고 뒤의 단어들은 가린 채 빈 칸의 정답이 될 수 있는 세 개의 선택지 중 고르는 것은 뒤의 단어들까지 알고있는 상태보다 명백히 정답을 결정하기가 어렵다.\n",
    "\n",
    "RNN이 풀고자 하는 문제 중에서는 과거 시점의 입력 뿐만 아니라 미래 시점의 입력에 힌트가 있는 경우도 많다. 그래서 이전과 이후의 시점 모두를 고려해서 현재 시점의 예측을 더욱 정확하게 할 수 있도록 고안된 것이 양방향 RNN이다. 다음 그림은 1개의 은닉층을 갖는 양방향 RNN의 구조를 보여준다.\n",
    "\n",
    "![그림 12](./images/section1/그림_12.png)\n",
    "\n",
    "양방향 RNN은 하나의 출력값을 예측하기 위해 **기본적으로 두 개의 메모리 셀을 사용**한다. 첫번째 메모리 셀은 앞에서 배운 것처럼 앞 시점의 은닉 상태(Forward States) 를 전달받아 현재의 은닉 상태를 계산한다. 위의 그림에서는 주황색 메모리 셀에 해당된다. 두번째 메모리 셀은 앞에서 배운 것과는 다르다. **앞 시점의 은닉 상태가 아니라 뒤 시점의 은닉 상태(Backward States) 를 전달 받아 현재의 은닉 상태를 계산**한다. 입력 시퀀스를 반대 방향으로 읽는 것이다. 위의 그림에서는 초록색 메모리 셀에 해당된다. 그리고 이 두 개의 값 모두가 현재 시점의 출력층에서 출력값을 예측하기 위해 사용된다. 크.. 아이디어 좋다. \n",
    "\n",
    "양방향 RNN도 케라스에 Sequential API로 이미 정의되어 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ce22f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 10, 16)           224       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224\n",
      "Trainable params: 224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional,SimpleRNN\n",
    "\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "hidden_units = 8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c70686",
   "metadata": {},
   "source": [
    "양방향 RNN도 다수의 은닉층을 가질 수 있다. 아래의 그림은 양방향 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은(deep) 양방향 순환 신경망의 모습을 보여준다.\n",
    "\n",
    "![그림 13](./images/section1/그림_13.png)\n",
    "\n",
    "다른 인공 신경망 모델들도 마찬가지이지만, 은닉층을 무조건 추가한다고 해서 모델의 성능이 좋아지는 것은 아니다. 은닉층을 추가하면 학습할 수 있는 양이 많아지지만 반대로 훈련 데이터 또한 많은 양이 필요하다. 아래의 코드는 은닉층이 4개인 경우이다.\n",
    "\n",
    "```python\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True)))\n",
    "\n",
    "```\n",
    "\n",
    "MLP할때도 느꼈지만, add로 추가되면 따로 입력의 크기를 결정짓지 않아도 그대로 출력 값을 다음 layer의 입력으로 가져오는 것이 keras의 장점인 것 같다. 내 기억으론 pytorch는 일일이 입력했어야했던 것 같은데..\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "# 퀴즈\n",
    "---\n",
    "\n",
    "단어 집합(vocabulary)의 크기가 5000인 데이터가 있다. 이것을 Embedding layer를 사용해서 차원이 100인 embedding vector를 만들고, SimpleRNN을 사용해 은닉층이 3개지만 다 대 다로 쌓이다가 최종 출력이 은닉 상태 1개인 딥 RNN을 만들어라. 이 때, 각각의 은닉 상태는 모두 차원이 128이다. 그리고 Sigmoid로 이진 분류를 하는 모델을 생성하라.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be3747a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 100)         500000    \n",
      "                                                                 \n",
      " simple_rnn_24 (SimpleRNN)   (None, None, 128)         29312     \n",
      "                                                                 \n",
      " simple_rnn_25 (SimpleRNN)   (None, None, 128)         32896     \n",
      "                                                                 \n",
      " simple_rnn_26 (SimpleRNN)   (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,233\n",
      "Trainable params: 595,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN,Embedding,Dense\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_dim))\n",
    "model.add(SimpleRNN(hidden_size,return_sequences=True))\n",
    "model.add(SimpleRNN(hidden_size,return_sequences=True))\n",
    "model.add(SimpleRNN(hidden_size,return_sequences=False))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
